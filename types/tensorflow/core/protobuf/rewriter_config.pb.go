// Code generated by protoc-gen-gogo. DO NOT EDIT.
// source: tensorflow/core/protobuf/rewriter_config.proto

package protobuf

import (
	fmt "fmt"
	framework "github.com/Applifier/go-tensorflow/types/tensorflow/core/framework"
	proto "github.com/gogo/protobuf/proto"
	io "io"
	math "math"
	math_bits "math/bits"
)

// Reference imports to suppress errors if they are not otherwise used.
var _ = proto.Marshal
var _ = fmt.Errorf
var _ = math.Inf

// This is a compile-time assertion to ensure that this generated file
// is compatible with the proto package it is being compiled against.
// A compilation error at this line likely means your copy of the
// proto package needs to be updated.
const _ = proto.GoGoProtoPackageIsVersion3 // please upgrade the proto package

type RewriterConfig_Toggle int32

const (
	RewriterConfig_DEFAULT RewriterConfig_Toggle = 0
	RewriterConfig_ON      RewriterConfig_Toggle = 1
	RewriterConfig_OFF     RewriterConfig_Toggle = 2
	// Enable some aggressive optimizations that use assumptions that TF graphs
	// may break. For example, assume the shape of a placeholder matches its
	// actual feed.
	RewriterConfig_AGGRESSIVE RewriterConfig_Toggle = 3
)

var RewriterConfig_Toggle_name = map[int32]string{
	0: "DEFAULT",
	1: "ON",
	2: "OFF",
	3: "AGGRESSIVE",
}

var RewriterConfig_Toggle_value = map[string]int32{
	"DEFAULT":    0,
	"ON":         1,
	"OFF":        2,
	"AGGRESSIVE": 3,
}

func (x RewriterConfig_Toggle) String() string {
	return proto.EnumName(RewriterConfig_Toggle_name, int32(x))
}

func (RewriterConfig_Toggle) EnumDescriptor() ([]byte, []int) {
	return fileDescriptor_1dd7de60bf190bbb, []int{2, 0}
}

// Enum for layout conversion between NCHW and NHWC on CPU. Default is OFF.
type RewriterConfig_CpuLayout int32

const (
	RewriterConfig_NO_CONVERSION_ON_CPU RewriterConfig_CpuLayout = 0
	RewriterConfig_NCHW_TO_NHWC         RewriterConfig_CpuLayout = 1
	RewriterConfig_NHWC_TO_NCHW         RewriterConfig_CpuLayout = 2
)

var RewriterConfig_CpuLayout_name = map[int32]string{
	0: "NO_CONVERSION_ON_CPU",
	1: "NCHW_TO_NHWC",
	2: "NHWC_TO_NCHW",
}

var RewriterConfig_CpuLayout_value = map[string]int32{
	"NO_CONVERSION_ON_CPU": 0,
	"NCHW_TO_NHWC":         1,
	"NHWC_TO_NCHW":         2,
}

func (x RewriterConfig_CpuLayout) String() string {
	return proto.EnumName(RewriterConfig_CpuLayout_name, int32(x))
}

func (RewriterConfig_CpuLayout) EnumDescriptor() ([]byte, []int) {
	return fileDescriptor_1dd7de60bf190bbb, []int{2, 1}
}

// Enum controlling the number of times to run optimizers. The default is to
// run them twice.
type RewriterConfig_NumIterationsType int32

const (
	RewriterConfig_DEFAULT_NUM_ITERS RewriterConfig_NumIterationsType = 0
	RewriterConfig_ONE               RewriterConfig_NumIterationsType = 1
	RewriterConfig_TWO               RewriterConfig_NumIterationsType = 2
)

var RewriterConfig_NumIterationsType_name = map[int32]string{
	0: "DEFAULT_NUM_ITERS",
	1: "ONE",
	2: "TWO",
}

var RewriterConfig_NumIterationsType_value = map[string]int32{
	"DEFAULT_NUM_ITERS": 0,
	"ONE":               1,
	"TWO":               2,
}

func (x RewriterConfig_NumIterationsType) String() string {
	return proto.EnumName(RewriterConfig_NumIterationsType_name, int32(x))
}

func (RewriterConfig_NumIterationsType) EnumDescriptor() ([]byte, []int) {
	return fileDescriptor_1dd7de60bf190bbb, []int{2, 2}
}

type RewriterConfig_MemOptType int32

const (
	// The default setting (SCHEDULING and SWAPPING HEURISTICS only)
	RewriterConfig_DEFAULT_MEM_OPT RewriterConfig_MemOptType = 0
	// Disabled in the meta-optimizer.
	RewriterConfig_NO_MEM_OPT RewriterConfig_MemOptType = 1
	// Driven by manual op-level annotations.
	RewriterConfig_MANUAL RewriterConfig_MemOptType = 2
	// Swapping heuristic will move a tensor from the GPU to the CPU and move
	// it back when needed to reduce peak memory usage.
	RewriterConfig_SWAPPING_HEURISTICS RewriterConfig_MemOptType = 4
	// Recomputation heuristics will recompute ops (such as Relu activation)
	// during backprop instead of storing them, reducing peak memory usage.
	RewriterConfig_RECOMPUTATION_HEURISTICS RewriterConfig_MemOptType = 5
	// Scheduling will split big ops such as AddN and try to enforce a schedule
	// of the new computations that decreases peak memory usage.
	RewriterConfig_SCHEDULING_HEURISTICS RewriterConfig_MemOptType = 6
	// Use any combination of swapping and recomputation heuristics.
	RewriterConfig_HEURISTICS RewriterConfig_MemOptType = 3
)

var RewriterConfig_MemOptType_name = map[int32]string{
	0: "DEFAULT_MEM_OPT",
	1: "NO_MEM_OPT",
	2: "MANUAL",
	4: "SWAPPING_HEURISTICS",
	5: "RECOMPUTATION_HEURISTICS",
	6: "SCHEDULING_HEURISTICS",
	3: "HEURISTICS",
}

var RewriterConfig_MemOptType_value = map[string]int32{
	"DEFAULT_MEM_OPT":          0,
	"NO_MEM_OPT":               1,
	"MANUAL":                   2,
	"SWAPPING_HEURISTICS":      4,
	"RECOMPUTATION_HEURISTICS": 5,
	"SCHEDULING_HEURISTICS":    6,
	"HEURISTICS":               3,
}

func (x RewriterConfig_MemOptType) String() string {
	return proto.EnumName(RewriterConfig_MemOptType_name, int32(x))
}

func (RewriterConfig_MemOptType) EnumDescriptor() ([]byte, []int) {
	return fileDescriptor_1dd7de60bf190bbb, []int{2, 3}
}

type AutoParallelOptions struct {
	Enable      bool  `protobuf:"varint,1,opt,name=enable,proto3" json:"enable,omitempty"`
	NumReplicas int32 `protobuf:"varint,2,opt,name=num_replicas,json=numReplicas,proto3" json:"num_replicas,omitempty"`
}

func (m *AutoParallelOptions) Reset()         { *m = AutoParallelOptions{} }
func (m *AutoParallelOptions) String() string { return proto.CompactTextString(m) }
func (*AutoParallelOptions) ProtoMessage()    {}
func (*AutoParallelOptions) Descriptor() ([]byte, []int) {
	return fileDescriptor_1dd7de60bf190bbb, []int{0}
}
func (m *AutoParallelOptions) XXX_Unmarshal(b []byte) error {
	return m.Unmarshal(b)
}
func (m *AutoParallelOptions) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	if deterministic {
		return xxx_messageInfo_AutoParallelOptions.Marshal(b, m, deterministic)
	} else {
		b = b[:cap(b)]
		n, err := m.MarshalToSizedBuffer(b)
		if err != nil {
			return nil, err
		}
		return b[:n], nil
	}
}
func (m *AutoParallelOptions) XXX_Merge(src proto.Message) {
	xxx_messageInfo_AutoParallelOptions.Merge(m, src)
}
func (m *AutoParallelOptions) XXX_Size() int {
	return m.Size()
}
func (m *AutoParallelOptions) XXX_DiscardUnknown() {
	xxx_messageInfo_AutoParallelOptions.DiscardUnknown(m)
}

var xxx_messageInfo_AutoParallelOptions proto.InternalMessageInfo

func (m *AutoParallelOptions) GetEnable() bool {
	if m != nil {
		return m.Enable
	}
	return false
}

func (m *AutoParallelOptions) GetNumReplicas() int32 {
	if m != nil {
		return m.NumReplicas
	}
	return 0
}

type ScopedAllocatorOptions struct {
	// If present, only perform optimization for these ops.
	EnableOp []string `protobuf:"bytes,1,rep,name=enable_op,json=enableOp,proto3" json:"enable_op,omitempty"`
}

func (m *ScopedAllocatorOptions) Reset()         { *m = ScopedAllocatorOptions{} }
func (m *ScopedAllocatorOptions) String() string { return proto.CompactTextString(m) }
func (*ScopedAllocatorOptions) ProtoMessage()    {}
func (*ScopedAllocatorOptions) Descriptor() ([]byte, []int) {
	return fileDescriptor_1dd7de60bf190bbb, []int{1}
}
func (m *ScopedAllocatorOptions) XXX_Unmarshal(b []byte) error {
	return m.Unmarshal(b)
}
func (m *ScopedAllocatorOptions) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	if deterministic {
		return xxx_messageInfo_ScopedAllocatorOptions.Marshal(b, m, deterministic)
	} else {
		b = b[:cap(b)]
		n, err := m.MarshalToSizedBuffer(b)
		if err != nil {
			return nil, err
		}
		return b[:n], nil
	}
}
func (m *ScopedAllocatorOptions) XXX_Merge(src proto.Message) {
	xxx_messageInfo_ScopedAllocatorOptions.Merge(m, src)
}
func (m *ScopedAllocatorOptions) XXX_Size() int {
	return m.Size()
}
func (m *ScopedAllocatorOptions) XXX_DiscardUnknown() {
	xxx_messageInfo_ScopedAllocatorOptions.DiscardUnknown(m)
}

var xxx_messageInfo_ScopedAllocatorOptions proto.InternalMessageInfo

func (m *ScopedAllocatorOptions) GetEnableOp() []string {
	if m != nil {
		return m.EnableOp
	}
	return nil
}

type RewriterConfig struct {
	// CPU Conversion settings between NHCW and NCHW.
	CpuLayoutConversion RewriterConfig_CpuLayout `protobuf:"varint,50,opt,name=cpu_layout_conversion,json=cpuLayoutConversion,proto3,enum=tensorflow.RewriterConfig_CpuLayout" json:"cpu_layout_conversion,omitempty"`
	// Optimize tensor layouts (default is ON)
	// e.g. This will try to use NCHW layout on GPU which is faster.
	LayoutOptimizer RewriterConfig_Toggle `protobuf:"varint,1,opt,name=layout_optimizer,json=layoutOptimizer,proto3,enum=tensorflow.RewriterConfig_Toggle" json:"layout_optimizer,omitempty"`
	// Fold constants (default is ON)
	// Statically infer the value of tensors when possible, and materialize the
	// result using constants.
	ConstantFolding RewriterConfig_Toggle `protobuf:"varint,3,opt,name=constant_folding,json=constantFolding,proto3,enum=tensorflow.RewriterConfig_Toggle" json:"constant_folding,omitempty"`
	// Shape optimizations (default is ON)
	// Simplify computations made on shapes.
	ShapeOptimization RewriterConfig_Toggle `protobuf:"varint,13,opt,name=shape_optimization,json=shapeOptimization,proto3,enum=tensorflow.RewriterConfig_Toggle" json:"shape_optimization,omitempty"`
	// Remapping (default is ON)
	// Remap subgraphs onto more efficient implementations.
	Remapping RewriterConfig_Toggle `protobuf:"varint,14,opt,name=remapping,proto3,enum=tensorflow.RewriterConfig_Toggle" json:"remapping,omitempty"`
	// Common subgraph elimination (default is ON)
	// e.g. Simplify arithmetic ops; merge ops with same value (like constants).
	CommonSubgraphElimination RewriterConfig_Toggle `protobuf:"varint,24,opt,name=common_subgraph_elimination,json=commonSubgraphElimination,proto3,enum=tensorflow.RewriterConfig_Toggle" json:"common_subgraph_elimination,omitempty"`
	// Arithmetic optimizations (default is ON)
	// e.g. Simplify arithmetic ops; merge ops with same value (like constants).
	ArithmeticOptimization RewriterConfig_Toggle `protobuf:"varint,7,opt,name=arithmetic_optimization,json=arithmeticOptimization,proto3,enum=tensorflow.RewriterConfig_Toggle" json:"arithmetic_optimization,omitempty"`
	// Control dependency optimizations (default is ON).
	// Remove redundant control dependencies, which may enable other optimization.
	DependencyOptimization RewriterConfig_Toggle `protobuf:"varint,8,opt,name=dependency_optimization,json=dependencyOptimization,proto3,enum=tensorflow.RewriterConfig_Toggle" json:"dependency_optimization,omitempty"`
	// Loop optimizations (default is ON).
	LoopOptimization RewriterConfig_Toggle `protobuf:"varint,9,opt,name=loop_optimization,json=loopOptimization,proto3,enum=tensorflow.RewriterConfig_Toggle" json:"loop_optimization,omitempty"`
	// Function optimizations (default is ON).
	FunctionOptimization RewriterConfig_Toggle `protobuf:"varint,10,opt,name=function_optimization,json=functionOptimization,proto3,enum=tensorflow.RewriterConfig_Toggle" json:"function_optimization,omitempty"`
	// Strips debug-related nodes from the graph (off by default).
	DebugStripper RewriterConfig_Toggle `protobuf:"varint,11,opt,name=debug_stripper,json=debugStripper,proto3,enum=tensorflow.RewriterConfig_Toggle" json:"debug_stripper,omitempty"`
	// If true, don't remove unnecessary ops from the graph
	DisableModelPruning bool `protobuf:"varint,2,opt,name=disable_model_pruning,json=disableModelPruning,proto3" json:"disable_model_pruning,omitempty"`
	// Try to allocate some independent Op outputs contiguously in order to
	// merge or eliminate downstream Ops (off by default).
	ScopedAllocatorOptimization RewriterConfig_Toggle `protobuf:"varint,15,opt,name=scoped_allocator_optimization,json=scopedAllocatorOptimization,proto3,enum=tensorflow.RewriterConfig_Toggle" json:"scoped_allocator_optimization,omitempty"`
	// Force small ops onto the CPU (default is OFF).
	PinToHostOptimization RewriterConfig_Toggle `protobuf:"varint,18,opt,name=pin_to_host_optimization,json=pinToHostOptimization,proto3,enum=tensorflow.RewriterConfig_Toggle" json:"pin_to_host_optimization,omitempty"`
	// Enable the swap of kernel implementations based on the device placement
	// (default is ON).
	ImplementationSelector RewriterConfig_Toggle `protobuf:"varint,22,opt,name=implementation_selector,json=implementationSelector,proto3,enum=tensorflow.RewriterConfig_Toggle" json:"implementation_selector,omitempty"`
	// Optimize data types for CUDA (default is OFF).
	// This will try to use float16 on GPU which is faster.
	// Note that this can change the numerical stability of the graph and may
	// require the use of loss scaling to maintain model convergence.
	AutoMixedPrecision RewriterConfig_Toggle `protobuf:"varint,23,opt,name=auto_mixed_precision,json=autoMixedPrecision,proto3,enum=tensorflow.RewriterConfig_Toggle" json:"auto_mixed_precision,omitempty"`
	// Optimize data types for MKL (default is OFF).
	// This will try to use bfloat16 on CPUs, which is faster.
	// Note that this can change the numerical stability of the graph.
	AutoMixedPrecisionMkl RewriterConfig_Toggle `protobuf:"varint,25,opt,name=auto_mixed_precision_mkl,json=autoMixedPrecisionMkl,proto3,enum=tensorflow.RewriterConfig_Toggle" json:"auto_mixed_precision_mkl,omitempty"`
	// Disable the entire meta optimizer (off by default).
	DisableMetaOptimizer bool `protobuf:"varint,19,opt,name=disable_meta_optimizer,json=disableMetaOptimizer,proto3" json:"disable_meta_optimizer,omitempty"`
	// Optimizers registered by plugin (default is ON)
	UsePluginOptimizers RewriterConfig_Toggle `protobuf:"varint,28,opt,name=use_plugin_optimizers,json=usePluginOptimizers,proto3,enum=tensorflow.RewriterConfig_Toggle" json:"use_plugin_optimizers,omitempty"`
	// Controls how many times we run the optimizers in meta optimizer (default
	// is once).
	MetaOptimizerIterations RewriterConfig_NumIterationsType `protobuf:"varint,12,opt,name=meta_optimizer_iterations,json=metaOptimizerIterations,proto3,enum=tensorflow.RewriterConfig_NumIterationsType" json:"meta_optimizer_iterations,omitempty"`
	// The minimum number of nodes in a graph to optimizer. For smaller graphs,
	// optimization is skipped.
	// 0 means the system picks an appropriate number.
	// < 0 means do not skip optimization.
	MinGraphNodes int32 `protobuf:"varint,17,opt,name=min_graph_nodes,json=minGraphNodes,proto3" json:"min_graph_nodes,omitempty"`
	// Disable optimizations that assume compressed tensors. Note that this flag
	// is experimental and may be removed in the future.
	ExperimentalDisableCompressedTensorOptimization bool `protobuf:"varint,26,opt,name=experimental_disable_compressed_tensor_optimization,json=experimentalDisableCompressedTensorOptimization,proto3" json:"experimental_disable_compressed_tensor_optimization,omitempty"`
	// Disable folding quantization emulation ops such as FakeQuantWithMinMax* and
	// QuantizeAndDequantize*. Some compilers (e.g. the TF-to-tflite converter)
	// have to extract quantization configs (e.g. min/max range, number of bits,
	// and per-channel) from the quantization emulation ops. Note that this flag
	// is experimental and may be removed in the future. See b/174138564 for more
	// details.
	ExperimentalDisableFoldingQuantizationEmulation bool `protobuf:"varint,27,opt,name=experimental_disable_folding_quantization_emulation,json=experimentalDisableFoldingQuantizationEmulation,proto3" json:"experimental_disable_folding_quantization_emulation,omitempty"`
	// Configures memory optimization passes through the meta-optimizer. Has no
	// effect on manually requested memory optimization passes in the optimizers
	// field.
	MemoryOptimization RewriterConfig_MemOptType `protobuf:"varint,4,opt,name=memory_optimization,json=memoryOptimization,proto3,enum=tensorflow.RewriterConfig_MemOptType" json:"memory_optimization,omitempty"`
	// A node name scope for node names which are valid outputs of recomputations.
	// Inputs to nodes that match this scope may be recomputed (subject either to
	// manual annotation of those input nodes or to manual annotation and
	// heuristics depending on memory_optimization), but the nodes themselves will
	// not be recomputed. This matches any sub-scopes as well, meaning the scope
	// can appear not just as a top-level scope. For example, if the value is
	// "gradients/", the default, it will match node name "gradients/foo",
	// "foo/gradients/bar", but not "foo_gradients/"
	MemoryOptimizerTargetNodeNameScope string `protobuf:"bytes,6,opt,name=memory_optimizer_target_node_name_scope,json=memoryOptimizerTargetNodeNameScope,proto3" json:"memory_optimizer_target_node_name_scope,omitempty"`
	// Maximum number of milliseconds to spend optimizing a single graph before
	// timing out. If equal to 0 the system picks a default (currently 5 minutes).
	// If less than 0 the optimizer will never time out.
	MetaOptimizerTimeoutMs int64 `protobuf:"varint,20,opt,name=meta_optimizer_timeout_ms,json=metaOptimizerTimeoutMs,proto3" json:"meta_optimizer_timeout_ms,omitempty"`
	// Configures AutoParallel optimization passes either through the
	// meta-optimizer or when manually specified through the optimizers field.
	AutoParallel *AutoParallelOptions `protobuf:"bytes,5,opt,name=auto_parallel,json=autoParallel,proto3" json:"auto_parallel,omitempty"`
	// If true, any optimization pass failing will cause the MetaOptimizer to
	// stop with an error. By default - or when set to false, failing passes are
	// skipped silently.
	FailOnOptimizerErrors bool                    `protobuf:"varint,21,opt,name=fail_on_optimizer_errors,json=failOnOptimizerErrors,proto3" json:"fail_on_optimizer_errors,omitempty"`
	ScopedAllocatorOpts   *ScopedAllocatorOptions `protobuf:"bytes,16,opt,name=scoped_allocator_opts,json=scopedAllocatorOpts,proto3" json:"scoped_allocator_opts,omitempty"`
	// If non-empty, will use this as an alternative way to specify a list of
	// optimizations to turn on and the order of the optimizations (replacing the
	// meta-optimizer).
	//
	// Of the RewriterConfig options, only the AutoParallel configuration options
	// (the auto_parallel field) apply to manually requested optimization passes
	// ("autoparallel"). Memory optimization passes ("memory") invoked here are
	// not configurable (in contrast to memory optimization passes through the
	// meta-optimizer) and act only on manual op annotations.
	//
	// Custom optimizers (see custom_optimizers) that are not part of this
	// schedule will be run after - in the order that they were specified.
	Optimizers []string `protobuf:"bytes,100,rep,name=optimizers,proto3" json:"optimizers,omitempty"`
	// list of CustomGraphOptimizers to apply.
	CustomOptimizers []*RewriterConfig_CustomGraphOptimizer `protobuf:"bytes,200,rep,name=custom_optimizers,json=customOptimizers,proto3" json:"custom_optimizers,omitempty"`
	// VerifierConfig specifying the verifiers to be run after every optimizer.
	InterOptimizerVerifierConfig *VerifierConfig `protobuf:"bytes,300,opt,name=inter_optimizer_verifier_config,json=interOptimizerVerifierConfig,proto3" json:"inter_optimizer_verifier_config,omitempty"`
	// VerifierConfig specifying the verifiers to be run at the end, after all
	// optimizers have run.
	PostOptimizationVerifierConfig *VerifierConfig `protobuf:"bytes,301,opt,name=post_optimization_verifier_config,json=postOptimizationVerifierConfig,proto3" json:"post_optimization_verifier_config,omitempty"`
}

func (m *RewriterConfig) Reset()         { *m = RewriterConfig{} }
func (m *RewriterConfig) String() string { return proto.CompactTextString(m) }
func (*RewriterConfig) ProtoMessage()    {}
func (*RewriterConfig) Descriptor() ([]byte, []int) {
	return fileDescriptor_1dd7de60bf190bbb, []int{2}
}
func (m *RewriterConfig) XXX_Unmarshal(b []byte) error {
	return m.Unmarshal(b)
}
func (m *RewriterConfig) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	if deterministic {
		return xxx_messageInfo_RewriterConfig.Marshal(b, m, deterministic)
	} else {
		b = b[:cap(b)]
		n, err := m.MarshalToSizedBuffer(b)
		if err != nil {
			return nil, err
		}
		return b[:n], nil
	}
}
func (m *RewriterConfig) XXX_Merge(src proto.Message) {
	xxx_messageInfo_RewriterConfig.Merge(m, src)
}
func (m *RewriterConfig) XXX_Size() int {
	return m.Size()
}
func (m *RewriterConfig) XXX_DiscardUnknown() {
	xxx_messageInfo_RewriterConfig.DiscardUnknown(m)
}

var xxx_messageInfo_RewriterConfig proto.InternalMessageInfo

func (m *RewriterConfig) GetCpuLayoutConversion() RewriterConfig_CpuLayout {
	if m != nil {
		return m.CpuLayoutConversion
	}
	return RewriterConfig_NO_CONVERSION_ON_CPU
}

func (m *RewriterConfig) GetLayoutOptimizer() RewriterConfig_Toggle {
	if m != nil {
		return m.LayoutOptimizer
	}
	return RewriterConfig_DEFAULT
}

func (m *RewriterConfig) GetConstantFolding() RewriterConfig_Toggle {
	if m != nil {
		return m.ConstantFolding
	}
	return RewriterConfig_DEFAULT
}

func (m *RewriterConfig) GetShapeOptimization() RewriterConfig_Toggle {
	if m != nil {
		return m.ShapeOptimization
	}
	return RewriterConfig_DEFAULT
}

func (m *RewriterConfig) GetRemapping() RewriterConfig_Toggle {
	if m != nil {
		return m.Remapping
	}
	return RewriterConfig_DEFAULT
}

func (m *RewriterConfig) GetCommonSubgraphElimination() RewriterConfig_Toggle {
	if m != nil {
		return m.CommonSubgraphElimination
	}
	return RewriterConfig_DEFAULT
}

func (m *RewriterConfig) GetArithmeticOptimization() RewriterConfig_Toggle {
	if m != nil {
		return m.ArithmeticOptimization
	}
	return RewriterConfig_DEFAULT
}

func (m *RewriterConfig) GetDependencyOptimization() RewriterConfig_Toggle {
	if m != nil {
		return m.DependencyOptimization
	}
	return RewriterConfig_DEFAULT
}

func (m *RewriterConfig) GetLoopOptimization() RewriterConfig_Toggle {
	if m != nil {
		return m.LoopOptimization
	}
	return RewriterConfig_DEFAULT
}

func (m *RewriterConfig) GetFunctionOptimization() RewriterConfig_Toggle {
	if m != nil {
		return m.FunctionOptimization
	}
	return RewriterConfig_DEFAULT
}

func (m *RewriterConfig) GetDebugStripper() RewriterConfig_Toggle {
	if m != nil {
		return m.DebugStripper
	}
	return RewriterConfig_DEFAULT
}

func (m *RewriterConfig) GetDisableModelPruning() bool {
	if m != nil {
		return m.DisableModelPruning
	}
	return false
}

func (m *RewriterConfig) GetScopedAllocatorOptimization() RewriterConfig_Toggle {
	if m != nil {
		return m.ScopedAllocatorOptimization
	}
	return RewriterConfig_DEFAULT
}

func (m *RewriterConfig) GetPinToHostOptimization() RewriterConfig_Toggle {
	if m != nil {
		return m.PinToHostOptimization
	}
	return RewriterConfig_DEFAULT
}

func (m *RewriterConfig) GetImplementationSelector() RewriterConfig_Toggle {
	if m != nil {
		return m.ImplementationSelector
	}
	return RewriterConfig_DEFAULT
}

func (m *RewriterConfig) GetAutoMixedPrecision() RewriterConfig_Toggle {
	if m != nil {
		return m.AutoMixedPrecision
	}
	return RewriterConfig_DEFAULT
}

func (m *RewriterConfig) GetAutoMixedPrecisionMkl() RewriterConfig_Toggle {
	if m != nil {
		return m.AutoMixedPrecisionMkl
	}
	return RewriterConfig_DEFAULT
}

func (m *RewriterConfig) GetDisableMetaOptimizer() bool {
	if m != nil {
		return m.DisableMetaOptimizer
	}
	return false
}

func (m *RewriterConfig) GetUsePluginOptimizers() RewriterConfig_Toggle {
	if m != nil {
		return m.UsePluginOptimizers
	}
	return RewriterConfig_DEFAULT
}

func (m *RewriterConfig) GetMetaOptimizerIterations() RewriterConfig_NumIterationsType {
	if m != nil {
		return m.MetaOptimizerIterations
	}
	return RewriterConfig_DEFAULT_NUM_ITERS
}

func (m *RewriterConfig) GetMinGraphNodes() int32 {
	if m != nil {
		return m.MinGraphNodes
	}
	return 0
}

func (m *RewriterConfig) GetExperimentalDisableCompressedTensorOptimization() bool {
	if m != nil {
		return m.ExperimentalDisableCompressedTensorOptimization
	}
	return false
}

func (m *RewriterConfig) GetExperimentalDisableFoldingQuantizationEmulation() bool {
	if m != nil {
		return m.ExperimentalDisableFoldingQuantizationEmulation
	}
	return false
}

func (m *RewriterConfig) GetMemoryOptimization() RewriterConfig_MemOptType {
	if m != nil {
		return m.MemoryOptimization
	}
	return RewriterConfig_DEFAULT_MEM_OPT
}

func (m *RewriterConfig) GetMemoryOptimizerTargetNodeNameScope() string {
	if m != nil {
		return m.MemoryOptimizerTargetNodeNameScope
	}
	return ""
}

func (m *RewriterConfig) GetMetaOptimizerTimeoutMs() int64 {
	if m != nil {
		return m.MetaOptimizerTimeoutMs
	}
	return 0
}

func (m *RewriterConfig) GetAutoParallel() *AutoParallelOptions {
	if m != nil {
		return m.AutoParallel
	}
	return nil
}

func (m *RewriterConfig) GetFailOnOptimizerErrors() bool {
	if m != nil {
		return m.FailOnOptimizerErrors
	}
	return false
}

func (m *RewriterConfig) GetScopedAllocatorOpts() *ScopedAllocatorOptions {
	if m != nil {
		return m.ScopedAllocatorOpts
	}
	return nil
}

func (m *RewriterConfig) GetOptimizers() []string {
	if m != nil {
		return m.Optimizers
	}
	return nil
}

func (m *RewriterConfig) GetCustomOptimizers() []*RewriterConfig_CustomGraphOptimizer {
	if m != nil {
		return m.CustomOptimizers
	}
	return nil
}

func (m *RewriterConfig) GetInterOptimizerVerifierConfig() *VerifierConfig {
	if m != nil {
		return m.InterOptimizerVerifierConfig
	}
	return nil
}

func (m *RewriterConfig) GetPostOptimizationVerifierConfig() *VerifierConfig {
	if m != nil {
		return m.PostOptimizationVerifierConfig
	}
	return nil
}

// Message to describe custom graph optimizer and its parameters
type RewriterConfig_CustomGraphOptimizer struct {
	Name         string                          `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	ParameterMap map[string]*framework.AttrValue `protobuf:"bytes,2,rep,name=parameter_map,json=parameterMap,proto3" json:"parameter_map,omitempty" protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
}

func (m *RewriterConfig_CustomGraphOptimizer) Reset()         { *m = RewriterConfig_CustomGraphOptimizer{} }
func (m *RewriterConfig_CustomGraphOptimizer) String() string { return proto.CompactTextString(m) }
func (*RewriterConfig_CustomGraphOptimizer) ProtoMessage()    {}
func (*RewriterConfig_CustomGraphOptimizer) Descriptor() ([]byte, []int) {
	return fileDescriptor_1dd7de60bf190bbb, []int{2, 0}
}
func (m *RewriterConfig_CustomGraphOptimizer) XXX_Unmarshal(b []byte) error {
	return m.Unmarshal(b)
}
func (m *RewriterConfig_CustomGraphOptimizer) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	if deterministic {
		return xxx_messageInfo_RewriterConfig_CustomGraphOptimizer.Marshal(b, m, deterministic)
	} else {
		b = b[:cap(b)]
		n, err := m.MarshalToSizedBuffer(b)
		if err != nil {
			return nil, err
		}
		return b[:n], nil
	}
}
func (m *RewriterConfig_CustomGraphOptimizer) XXX_Merge(src proto.Message) {
	xxx_messageInfo_RewriterConfig_CustomGraphOptimizer.Merge(m, src)
}
func (m *RewriterConfig_CustomGraphOptimizer) XXX_Size() int {
	return m.Size()
}
func (m *RewriterConfig_CustomGraphOptimizer) XXX_DiscardUnknown() {
	xxx_messageInfo_RewriterConfig_CustomGraphOptimizer.DiscardUnknown(m)
}

var xxx_messageInfo_RewriterConfig_CustomGraphOptimizer proto.InternalMessageInfo

func (m *RewriterConfig_CustomGraphOptimizer) GetName() string {
	if m != nil {
		return m.Name
	}
	return ""
}

func (m *RewriterConfig_CustomGraphOptimizer) GetParameterMap() map[string]*framework.AttrValue {
	if m != nil {
		return m.ParameterMap
	}
	return nil
}

func init() {
	proto.RegisterEnum("tensorflow.RewriterConfig_Toggle", RewriterConfig_Toggle_name, RewriterConfig_Toggle_value)
	proto.RegisterEnum("tensorflow.RewriterConfig_CpuLayout", RewriterConfig_CpuLayout_name, RewriterConfig_CpuLayout_value)
	proto.RegisterEnum("tensorflow.RewriterConfig_NumIterationsType", RewriterConfig_NumIterationsType_name, RewriterConfig_NumIterationsType_value)
	proto.RegisterEnum("tensorflow.RewriterConfig_MemOptType", RewriterConfig_MemOptType_name, RewriterConfig_MemOptType_value)
	proto.RegisterType((*AutoParallelOptions)(nil), "tensorflow.AutoParallelOptions")
	proto.RegisterType((*ScopedAllocatorOptions)(nil), "tensorflow.ScopedAllocatorOptions")
	proto.RegisterType((*RewriterConfig)(nil), "tensorflow.RewriterConfig")
	proto.RegisterType((*RewriterConfig_CustomGraphOptimizer)(nil), "tensorflow.RewriterConfig.CustomGraphOptimizer")
	proto.RegisterMapType((map[string]*framework.AttrValue)(nil), "tensorflow.RewriterConfig.CustomGraphOptimizer.ParameterMapEntry")
}

func init() {
	proto.RegisterFile("tensorflow/core/protobuf/rewriter_config.proto", fileDescriptor_1dd7de60bf190bbb)
}

var fileDescriptor_1dd7de60bf190bbb = []byte{
	// 1483 bytes of a gzipped FileDescriptorProto
	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0x94, 0x57, 0xd9, 0x6e, 0xdb, 0x46,
	0x17, 0x36, 0xe5, 0xd8, 0x89, 0x8f, 0x37, 0x7a, 0x64, 0xd9, 0xb4, 0x9d, 0x5f, 0x71, 0x84, 0x7f,
	0x31, 0xfe, 0xc5, 0x02, 0x9c, 0x7f, 0x47, 0x81, 0x42, 0x91, 0x65, 0x4b, 0x80, 0x45, 0xaa, 0x94,
	0x64, 0x17, 0x01, 0x8a, 0x01, 0x4d, 0x8d, 0x64, 0xc2, 0x1c, 0xce, 0x74, 0x38, 0x4c, 0xe2, 0x3e,
	0x45, 0xde, 0xa0, 0x2f, 0xd0, 0xbe, 0x47, 0x2e, 0x73, 0xd9, 0xcb, 0x22, 0xb9, 0xeb, 0x13, 0xf4,
	0xae, 0x05, 0x87, 0x5a, 0x48, 0x79, 0xa9, 0x7a, 0x47, 0x9e, 0x33, 0xdf, 0x77, 0x96, 0x39, 0x73,
	0xe6, 0x0c, 0x1c, 0x4a, 0x12, 0x84, 0x4c, 0xf4, 0x7d, 0xf6, 0xa6, 0xec, 0x32, 0x41, 0xca, 0x5c,
	0x30, 0xc9, 0x2e, 0xa3, 0x7e, 0x59, 0x90, 0x37, 0xc2, 0x93, 0x44, 0x60, 0x97, 0x05, 0x7d, 0x6f,
	0x70, 0xa8, 0x14, 0x08, 0x26, 0xeb, 0x77, 0xff, 0x3a, 0x8d, 0xed, 0x0b, 0x87, 0x92, 0x37, 0x4c,
	0x5c, 0x97, 0x1d, 0x29, 0x05, 0x7e, 0xed, 0xf8, 0x11, 0x49, 0x70, 0xbb, 0xf7, 0xdb, 0x79, 0x4d,
	0x84, 0xd7, 0xf7, 0xa6, 0xec, 0x94, 0x5a, 0x90, 0xaf, 0x44, 0x92, 0xb5, 0x1c, 0xe1, 0xf8, 0x3e,
	0xf1, 0x2d, 0x2e, 0x3d, 0x16, 0x84, 0x68, 0x0b, 0x16, 0x49, 0xe0, 0x5c, 0xfa, 0xc4, 0xd0, 0xf6,
	0xb5, 0x83, 0x27, 0xf6, 0xf0, 0x0f, 0x3d, 0x87, 0x95, 0x20, 0xa2, 0x58, 0x10, 0xee, 0x7b, 0xae,
	0x13, 0x1a, 0xb9, 0x7d, 0xed, 0x60, 0xc1, 0x5e, 0x0e, 0x22, 0x6a, 0x0f, 0x45, 0xa5, 0x7f, 0xc1,
	0x56, 0xdb, 0x65, 0x9c, 0xf4, 0x2a, 0xbe, 0xcf, 0x5c, 0x47, 0x32, 0x31, 0x22, 0xdd, 0x83, 0xa5,
	0x84, 0x06, 0x33, 0x6e, 0x68, 0xfb, 0xf3, 0x07, 0x4b, 0xf6, 0x93, 0x44, 0x60, 0xf1, 0xd2, 0x2f,
	0x3b, 0xb0, 0x66, 0x0f, 0x53, 0x51, 0x55, 0x1e, 0xa2, 0x2f, 0xa1, 0xe0, 0xf2, 0x08, 0xfb, 0xce,
	0x0d, 0x8b, 0x64, 0xec, 0xf6, 0x6b, 0x22, 0x42, 0x8f, 0x05, 0xc6, 0xd1, 0xbe, 0x76, 0xb0, 0x76,
	0xf4, 0xc7, 0x54, 0xac, 0x87, 0x59, 0xe8, 0x61, 0x95, 0x47, 0x67, 0x0a, 0x66, 0xe7, 0xdd, 0xd1,
	0x67, 0x75, 0x4c, 0x80, 0xce, 0x40, 0x1f, 0xb2, 0x32, 0x2e, 0x3d, 0xea, 0x7d, 0x43, 0x84, 0x0a,
	0x74, 0xed, 0xe8, 0xf9, 0x03, 0xa4, 0x1d, 0x36, 0x18, 0xf8, 0xc4, 0x5e, 0x4f, 0xa0, 0xd6, 0x08,
	0x19, 0xb3, 0xb9, 0x2c, 0x08, 0xa5, 0x13, 0x48, 0xdc, 0x67, 0x7e, 0xcf, 0x0b, 0x06, 0xc6, 0xfc,
	0xcc, 0x6c, 0x23, 0xe8, 0x49, 0x82, 0x44, 0x2d, 0x40, 0xe1, 0x95, 0xc3, 0xc9, 0xc8, 0x35, 0x27,
	0x4e, 0x9e, 0xb1, 0x3a, 0x2b, 0xdf, 0x86, 0x02, 0x5b, 0x29, 0x2c, 0xfa, 0x1c, 0x96, 0x04, 0xa1,
	0x0e, 0xe7, 0xb1, 0x63, 0x6b, 0xb3, 0x12, 0x4d, 0x30, 0xc8, 0x81, 0x3d, 0x97, 0x51, 0xca, 0x02,
	0x1c, 0x46, 0x97, 0x03, 0xe1, 0xf0, 0x2b, 0x4c, 0x7c, 0x8f, 0x7a, 0x41, 0xe2, 0x9b, 0x31, 0x2b,
	0xe5, 0x4e, 0xc2, 0xd2, 0x1e, 0x92, 0xd4, 0x26, 0x1c, 0xe8, 0x15, 0x6c, 0x3b, 0xc2, 0x93, 0x57,
	0x94, 0x48, 0xcf, 0xcd, 0x86, 0xfe, 0x78, 0x56, 0xfa, 0xad, 0x09, 0x43, 0x26, 0xfe, 0x57, 0xb0,
	0xdd, 0x23, 0x9c, 0x04, 0x3d, 0x12, 0xb8, 0x37, 0x59, 0xee, 0x27, 0x33, 0x73, 0x4f, 0x18, 0x32,
	0xdc, 0x26, 0x6c, 0xf8, 0x8c, 0xf1, 0x2c, 0xeb, 0xd2, 0xac, 0xac, 0x7a, 0x8c, 0xcd, 0xf0, 0x9d,
	0x43, 0xa1, 0x1f, 0x05, 0x6e, 0xfc, 0x9d, 0xe5, 0x84, 0x59, 0x39, 0x37, 0x47, 0xf8, 0x0c, 0x6f,
	0x1d, 0xd6, 0x7a, 0xe4, 0x32, 0x1a, 0xe0, 0x50, 0x0a, 0x8f, 0x73, 0x22, 0x8c, 0xe5, 0x59, 0x09,
	0x57, 0x15, 0xb0, 0x3d, 0xc4, 0xa1, 0x23, 0x28, 0xf4, 0xbc, 0x50, 0x1d, 0x63, 0xca, 0x7a, 0xc4,
	0xc7, 0x5c, 0x44, 0x41, 0x5c, 0x59, 0x39, 0xd5, 0x29, 0xf2, 0x43, 0x65, 0x33, 0xd6, 0xb5, 0x12,
	0x15, 0x22, 0xf0, 0x87, 0x50, 0xf5, 0x04, 0xec, 0x8c, 0x9a, 0x42, 0x36, 0xba, 0xf5, 0x59, 0x9d,
	0xd9, 0x0b, 0x6f, 0xf7, 0x96, 0xd4, 0x46, 0x1b, 0xdc, 0x0b, 0xb0, 0x64, 0xf8, 0x8a, 0x85, 0x32,
	0x6b, 0x01, 0xcd, 0x6a, 0xa1, 0xc0, 0xbd, 0xa0, 0xc3, 0xea, 0x2c, 0x94, 0xd3, 0x45, 0xe4, 0x51,
	0xee, 0x13, 0x4a, 0x02, 0xa9, 0x24, 0x38, 0x24, 0x3e, 0x71, 0x25, 0x13, 0xc6, 0xd6, 0xcc, 0x45,
	0x94, 0x65, 0x68, 0x0f, 0x09, 0x50, 0x1b, 0x36, 0x9d, 0x48, 0x32, 0x4c, 0xbd, 0xb7, 0xa4, 0x87,
	0xb9, 0x20, 0xae, 0xa7, 0xfa, 0xdc, 0xf6, 0xac, 0xc4, 0x28, 0x86, 0x37, 0x63, 0x74, 0x6b, 0x04,
	0x8e, 0x93, 0x71, 0x17, 0x29, 0xa6, 0xd7, 0xbe, 0xb1, 0x33, 0x73, 0x32, 0x6e, 0x13, 0x37, 0xaf,
	0x7d, 0xf4, 0x4f, 0xd8, 0x1a, 0xd7, 0x00, 0x91, 0x4e, 0xaa, 0x8b, 0xe6, 0x55, 0x11, 0x6c, 0x8e,
	0x8a, 0x80, 0x48, 0x67, 0xd2, 0x27, 0xbb, 0x50, 0x88, 0x42, 0x82, 0xb9, 0x1f, 0x0d, 0xbc, 0x60,
	0x82, 0x09, 0x8d, 0xa7, 0xb3, 0xba, 0x93, 0x8f, 0x42, 0xd2, 0x52, 0xf0, 0x31, 0x6b, 0x88, 0xae,
	0x60, 0x27, 0xeb, 0x04, 0x8e, 0x81, 0x2a, 0xc3, 0xa1, 0xb1, 0xa2, 0xa8, 0xff, 0xfe, 0x00, 0xb5,
	0x19, 0xd1, 0xc6, 0x78, 0x7d, 0xe7, 0x86, 0x13, 0x7b, 0x9b, 0xa6, 0xdd, 0x9e, 0x28, 0xd1, 0x9f,
	0x61, 0x9d, 0x7a, 0x01, 0x4e, 0x3a, 0x60, 0xc0, 0x7a, 0x24, 0x34, 0x36, 0xd4, 0x05, 0xb8, 0x4a,
	0xbd, 0xe0, 0x34, 0x96, 0x9a, 0xb1, 0x10, 0xf9, 0xf0, 0x82, 0xbc, 0xe5, 0x44, 0x78, 0x6a, 0xab,
	0x7d, 0x3c, 0xca, 0x95, 0xcb, 0x28, 0x17, 0x24, 0x0c, 0x49, 0x0f, 0x27, 0xfe, 0x64, 0x4b, 0x74,
	0x57, 0xe5, 0xae, 0x9c, 0x86, 0x1e, 0x27, 0xc8, 0xea, 0x18, 0xd8, 0x51, 0xb8, 0x4c, 0x65, 0xde,
	0x67, 0x6d, 0x78, 0x15, 0xe1, 0xaf, 0x23, 0x27, 0x90, 0xc3, 0xc5, 0x98, 0xd0, 0xc8, 0x4f, 0xac,
	0xed, 0xdd, 0x6b, 0x6d, 0x78, 0x13, 0x7d, 0x91, 0xc2, 0xd5, 0x46, 0x30, 0x74, 0x0e, 0x79, 0x4a,
	0x28, 0x13, 0x53, 0x8d, 0xf4, 0x91, 0xca, 0xf3, 0x9f, 0x1e, 0xc8, 0x73, 0x93, 0x50, 0x8b, 0x4b,
	0x95, 0x60, 0x94, 0x30, 0x64, 0xa2, 0x68, 0xc3, 0x5f, 0xb2, 0xbc, 0x44, 0x60, 0xe9, 0x88, 0x01,
	0x91, 0x2a, 0xd3, 0x38, 0x70, 0x28, 0xc1, 0xea, 0xf0, 0x1b, 0x8b, 0xfb, 0xda, 0xc1, 0x92, 0x5d,
	0xca, 0x90, 0x10, 0xd1, 0x51, 0x8b, 0xe3, 0x0d, 0x30, 0x1d, 0x4a, 0xd4, 0x08, 0x82, 0xfe, 0x77,
	0xab, 0x34, 0xa4, 0x47, 0x49, 0x7c, 0xef, 0xd3, 0xd0, 0xd8, 0xdc, 0xd7, 0x0e, 0xe6, 0xed, 0xad,
	0xcc, 0x66, 0x77, 0x12, 0x75, 0x33, 0x44, 0xc7, 0xb0, 0xaa, 0x8e, 0x0f, 0x1f, 0x4e, 0x46, 0xc6,
	0xc2, 0xbe, 0x76, 0xb0, 0x7c, 0xf4, 0x2c, 0x1d, 0xe1, 0x1d, 0x93, 0x93, 0xbd, 0xe2, 0xa4, 0x84,
	0xe8, 0x3f, 0x60, 0xf4, 0x1d, 0xcf, 0xc7, 0x2c, 0x55, 0xef, 0x98, 0x08, 0xc1, 0x44, 0x68, 0x14,
	0xd4, 0x06, 0x14, 0x62, 0xbd, 0x35, 0xa9, 0xe7, 0x9a, 0x52, 0xc6, 0xf7, 0xc0, 0x5d, 0x1d, 0x33,
	0x34, 0x74, 0xe5, 0x46, 0x29, 0xed, 0xc6, 0xdd, 0xe3, 0x96, 0x9d, 0xbf, 0xdd, 0x2a, 0x43, 0x54,
	0x04, 0x48, 0x1d, 0xbc, 0x9e, 0x1a, 0xc2, 0x52, 0x12, 0xf4, 0x15, 0x6c, 0xb8, 0x51, 0x28, 0x19,
	0x4d, 0x9f, 0xcf, 0xf7, 0xf1, 0xb0, 0xb6, 0x7c, 0x54, 0x7e, 0x68, 0xe0, 0x52, 0x20, 0x75, 0x06,
	0xc6, 0xa1, 0xd8, 0x7a, 0x42, 0x95, 0x3a, 0xab, 0x97, 0xf0, 0xcc, 0x0b, 0xe2, 0x61, 0x77, 0x92,
	0x8d, 0xa9, 0xb9, 0xd4, 0xf8, 0x2e, 0xa7, 0x22, 0x4c, 0x4f, 0xb2, 0x87, 0xe7, 0xc3, 0x35, 0x89,
	0x31, 0xfb, 0xa9, 0xe2, 0x18, 0xd3, 0x66, 0xb5, 0xa8, 0x0f, 0xcf, 0xf9, 0x74, 0xfb, 0xbf, 0x65,
	0xe5, 0xfb, 0xdf, 0xb6, 0x52, 0xe4, 0x53, 0x77, 0x40, 0x56, 0xbf, 0xfb, 0x93, 0x06, 0x9b, 0x77,
	0x85, 0x8d, 0x10, 0x3c, 0x8a, 0xab, 0x55, 0x4d, 0x94, 0x4b, 0xb6, 0xfa, 0x46, 0x7d, 0x58, 0x8d,
	0x2b, 0x89, 0x92, 0x38, 0x78, 0xea, 0x70, 0x23, 0xa7, 0x52, 0x5a, 0xf9, 0x9d, 0x29, 0x3d, 0x6c,
	0x8d, 0x48, 0x9a, 0x0e, 0xaf, 0x05, 0x52, 0xdc, 0xd8, 0x2b, 0x3c, 0x25, 0xda, 0x3d, 0x87, 0x8d,
	0x5b, 0x4b, 0x90, 0x0e, 0xf3, 0xd7, 0xe4, 0x66, 0xe8, 0x4f, 0xfc, 0x89, 0xfe, 0x06, 0x0b, 0xea,
	0xd5, 0x60, 0x24, 0x69, 0x28, 0x64, 0xaa, 0x5a, 0x4a, 0x71, 0x1e, 0x2b, 0xed, 0x64, 0xcd, 0xff,
	0x73, 0xff, 0xd5, 0x4a, 0xff, 0x86, 0xc5, 0xa4, 0x07, 0xa3, 0x65, 0x78, 0x7c, 0x5c, 0x3b, 0xa9,
	0x74, 0xcf, 0x3a, 0xfa, 0x1c, 0x5a, 0x84, 0x9c, 0x65, 0xea, 0x1a, 0x7a, 0x0c, 0xf3, 0xd6, 0xc9,
	0x89, 0x9e, 0x43, 0x6b, 0x00, 0x95, 0xd3, 0x53, 0xbb, 0xd6, 0x6e, 0x37, 0xce, 0x6b, 0xfa, 0x7c,
	0xa9, 0x01, 0x4b, 0xe3, 0x59, 0x1c, 0x19, 0xb0, 0x69, 0x5a, 0xb8, 0x6a, 0x99, 0xe7, 0x35, 0xbb,
	0xdd, 0xb0, 0x4c, 0x6c, 0x99, 0xb8, 0xda, 0xea, 0xea, 0x73, 0x48, 0x87, 0x15, 0xb3, 0x5a, 0xbf,
	0xc0, 0x1d, 0x0b, 0x9b, 0xf5, 0x8b, 0xaa, 0xae, 0x29, 0x49, 0xfd, 0xa2, 0xaa, 0x24, 0xd5, 0xfa,
	0x85, 0x9e, 0x2b, 0x7d, 0x06, 0x1b, 0xb7, 0x7a, 0x35, 0x2a, 0xc0, 0xc6, 0xd0, 0x1b, 0x6c, 0x76,
	0x9b, 0xb8, 0xd1, 0xa9, 0xd9, 0x6d, 0x7d, 0x4e, 0xf9, 0x63, 0xd6, 0x12, 0xc7, 0x3a, 0x17, 0x96,
	0x9e, 0x2b, 0x7d, 0xab, 0x01, 0x4c, 0x5a, 0x10, 0xca, 0xc3, 0xfa, 0x08, 0xd7, 0xac, 0x35, 0xb1,
	0xd5, 0x8a, 0xa3, 0x59, 0x03, 0x30, 0xad, 0xf1, 0xbf, 0x86, 0x00, 0x16, 0x9b, 0x15, 0xb3, 0x5b,
	0x39, 0xd3, 0x73, 0x68, 0x1b, 0xf2, 0xed, 0x8b, 0x4a, 0xab, 0xd5, 0x30, 0x4f, 0x71, 0xbd, 0xd6,
	0xb5, 0x1b, 0xed, 0x4e, 0xa3, 0xda, 0xd6, 0x1f, 0xa1, 0xa7, 0x60, 0xd8, 0xb5, 0xaa, 0xd5, 0x6c,
	0x75, 0x3b, 0x95, 0x4e, 0x1c, 0x54, 0x4a, 0xbb, 0x80, 0x76, 0xa0, 0xd0, 0xae, 0xd6, 0x6b, 0xc7,
	0xdd, 0xb3, 0x29, 0xe0, 0x62, 0x6c, 0x2d, 0xf5, 0x3f, 0xff, 0xf2, 0x9d, 0xf6, 0xfe, 0x63, 0x51,
	0xfb, 0xf0, 0xb1, 0xa8, 0xfd, 0xf8, 0xb1, 0xa8, 0xbd, 0xfb, 0x54, 0x9c, 0xfb, 0xf0, 0xa9, 0x38,
	0xf7, 0xc3, 0xa7, 0xe2, 0x1c, 0x18, 0x4c, 0x0c, 0xd2, 0x5b, 0x34, 0x7e, 0x00, 0xbe, 0xdc, 0xcc,
	0x16, 0x4d, 0x2b, 0x7e, 0xd4, 0x85, 0x2d, 0xed, 0x55, 0x65, 0xe0, 0xc9, 0xab, 0xe8, 0xf2, 0xd0,
	0x65, 0xb4, 0x5c, 0xe1, 0xdc, 0x57, 0x85, 0x5b, 0x1e, 0xb0, 0x7f, 0xa4, 0xde, 0x87, 0xf2, 0x86,
	0x93, 0xb0, 0x7c, 0xdf, 0x83, 0xf1, 0x67, 0x4d, 0xbb, 0x5c, 0x54, 0x3f, 0x2f, 0x7e, 0x0d, 0x00,
	0x00, 0xff, 0xff, 0xf7, 0xdb, 0xa7, 0x41, 0xbe, 0x0e, 0x00, 0x00,
}

func (m *AutoParallelOptions) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalToSizedBuffer(dAtA[:size])
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *AutoParallelOptions) MarshalTo(dAtA []byte) (int, error) {
	size := m.Size()
	return m.MarshalToSizedBuffer(dAtA[:size])
}

func (m *AutoParallelOptions) MarshalToSizedBuffer(dAtA []byte) (int, error) {
	i := len(dAtA)
	_ = i
	var l int
	_ = l
	if m.NumReplicas != 0 {
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.NumReplicas))
		i--
		dAtA[i] = 0x10
	}
	if m.Enable {
		i--
		if m.Enable {
			dAtA[i] = 1
		} else {
			dAtA[i] = 0
		}
		i--
		dAtA[i] = 0x8
	}
	return len(dAtA) - i, nil
}

func (m *ScopedAllocatorOptions) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalToSizedBuffer(dAtA[:size])
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *ScopedAllocatorOptions) MarshalTo(dAtA []byte) (int, error) {
	size := m.Size()
	return m.MarshalToSizedBuffer(dAtA[:size])
}

func (m *ScopedAllocatorOptions) MarshalToSizedBuffer(dAtA []byte) (int, error) {
	i := len(dAtA)
	_ = i
	var l int
	_ = l
	if len(m.EnableOp) > 0 {
		for iNdEx := len(m.EnableOp) - 1; iNdEx >= 0; iNdEx-- {
			i -= len(m.EnableOp[iNdEx])
			copy(dAtA[i:], m.EnableOp[iNdEx])
			i = encodeVarintRewriterConfig(dAtA, i, uint64(len(m.EnableOp[iNdEx])))
			i--
			dAtA[i] = 0xa
		}
	}
	return len(dAtA) - i, nil
}

func (m *RewriterConfig) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalToSizedBuffer(dAtA[:size])
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *RewriterConfig) MarshalTo(dAtA []byte) (int, error) {
	size := m.Size()
	return m.MarshalToSizedBuffer(dAtA[:size])
}

func (m *RewriterConfig) MarshalToSizedBuffer(dAtA []byte) (int, error) {
	i := len(dAtA)
	_ = i
	var l int
	_ = l
	if m.PostOptimizationVerifierConfig != nil {
		{
			size, err := m.PostOptimizationVerifierConfig.MarshalToSizedBuffer(dAtA[:i])
			if err != nil {
				return 0, err
			}
			i -= size
			i = encodeVarintRewriterConfig(dAtA, i, uint64(size))
		}
		i--
		dAtA[i] = 0x12
		i--
		dAtA[i] = 0xea
	}
	if m.InterOptimizerVerifierConfig != nil {
		{
			size, err := m.InterOptimizerVerifierConfig.MarshalToSizedBuffer(dAtA[:i])
			if err != nil {
				return 0, err
			}
			i -= size
			i = encodeVarintRewriterConfig(dAtA, i, uint64(size))
		}
		i--
		dAtA[i] = 0x12
		i--
		dAtA[i] = 0xe2
	}
	if len(m.CustomOptimizers) > 0 {
		for iNdEx := len(m.CustomOptimizers) - 1; iNdEx >= 0; iNdEx-- {
			{
				size, err := m.CustomOptimizers[iNdEx].MarshalToSizedBuffer(dAtA[:i])
				if err != nil {
					return 0, err
				}
				i -= size
				i = encodeVarintRewriterConfig(dAtA, i, uint64(size))
			}
			i--
			dAtA[i] = 0xc
			i--
			dAtA[i] = 0xc2
		}
	}
	if len(m.Optimizers) > 0 {
		for iNdEx := len(m.Optimizers) - 1; iNdEx >= 0; iNdEx-- {
			i -= len(m.Optimizers[iNdEx])
			copy(dAtA[i:], m.Optimizers[iNdEx])
			i = encodeVarintRewriterConfig(dAtA, i, uint64(len(m.Optimizers[iNdEx])))
			i--
			dAtA[i] = 0x6
			i--
			dAtA[i] = 0xa2
		}
	}
	if m.CpuLayoutConversion != 0 {
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.CpuLayoutConversion))
		i--
		dAtA[i] = 0x3
		i--
		dAtA[i] = 0x90
	}
	if m.UsePluginOptimizers != 0 {
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.UsePluginOptimizers))
		i--
		dAtA[i] = 0x1
		i--
		dAtA[i] = 0xe0
	}
	if m.ExperimentalDisableFoldingQuantizationEmulation {
		i--
		if m.ExperimentalDisableFoldingQuantizationEmulation {
			dAtA[i] = 1
		} else {
			dAtA[i] = 0
		}
		i--
		dAtA[i] = 0x1
		i--
		dAtA[i] = 0xd8
	}
	if m.ExperimentalDisableCompressedTensorOptimization {
		i--
		if m.ExperimentalDisableCompressedTensorOptimization {
			dAtA[i] = 1
		} else {
			dAtA[i] = 0
		}
		i--
		dAtA[i] = 0x1
		i--
		dAtA[i] = 0xd0
	}
	if m.AutoMixedPrecisionMkl != 0 {
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.AutoMixedPrecisionMkl))
		i--
		dAtA[i] = 0x1
		i--
		dAtA[i] = 0xc8
	}
	if m.CommonSubgraphElimination != 0 {
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.CommonSubgraphElimination))
		i--
		dAtA[i] = 0x1
		i--
		dAtA[i] = 0xc0
	}
	if m.AutoMixedPrecision != 0 {
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.AutoMixedPrecision))
		i--
		dAtA[i] = 0x1
		i--
		dAtA[i] = 0xb8
	}
	if m.ImplementationSelector != 0 {
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.ImplementationSelector))
		i--
		dAtA[i] = 0x1
		i--
		dAtA[i] = 0xb0
	}
	if m.FailOnOptimizerErrors {
		i--
		if m.FailOnOptimizerErrors {
			dAtA[i] = 1
		} else {
			dAtA[i] = 0
		}
		i--
		dAtA[i] = 0x1
		i--
		dAtA[i] = 0xa8
	}
	if m.MetaOptimizerTimeoutMs != 0 {
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.MetaOptimizerTimeoutMs))
		i--
		dAtA[i] = 0x1
		i--
		dAtA[i] = 0xa0
	}
	if m.DisableMetaOptimizer {
		i--
		if m.DisableMetaOptimizer {
			dAtA[i] = 1
		} else {
			dAtA[i] = 0
		}
		i--
		dAtA[i] = 0x1
		i--
		dAtA[i] = 0x98
	}
	if m.PinToHostOptimization != 0 {
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.PinToHostOptimization))
		i--
		dAtA[i] = 0x1
		i--
		dAtA[i] = 0x90
	}
	if m.MinGraphNodes != 0 {
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.MinGraphNodes))
		i--
		dAtA[i] = 0x1
		i--
		dAtA[i] = 0x88
	}
	if m.ScopedAllocatorOpts != nil {
		{
			size, err := m.ScopedAllocatorOpts.MarshalToSizedBuffer(dAtA[:i])
			if err != nil {
				return 0, err
			}
			i -= size
			i = encodeVarintRewriterConfig(dAtA, i, uint64(size))
		}
		i--
		dAtA[i] = 0x1
		i--
		dAtA[i] = 0x82
	}
	if m.ScopedAllocatorOptimization != 0 {
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.ScopedAllocatorOptimization))
		i--
		dAtA[i] = 0x78
	}
	if m.Remapping != 0 {
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.Remapping))
		i--
		dAtA[i] = 0x70
	}
	if m.ShapeOptimization != 0 {
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.ShapeOptimization))
		i--
		dAtA[i] = 0x68
	}
	if m.MetaOptimizerIterations != 0 {
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.MetaOptimizerIterations))
		i--
		dAtA[i] = 0x60
	}
	if m.DebugStripper != 0 {
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.DebugStripper))
		i--
		dAtA[i] = 0x58
	}
	if m.FunctionOptimization != 0 {
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.FunctionOptimization))
		i--
		dAtA[i] = 0x50
	}
	if m.LoopOptimization != 0 {
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.LoopOptimization))
		i--
		dAtA[i] = 0x48
	}
	if m.DependencyOptimization != 0 {
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.DependencyOptimization))
		i--
		dAtA[i] = 0x40
	}
	if m.ArithmeticOptimization != 0 {
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.ArithmeticOptimization))
		i--
		dAtA[i] = 0x38
	}
	if len(m.MemoryOptimizerTargetNodeNameScope) > 0 {
		i -= len(m.MemoryOptimizerTargetNodeNameScope)
		copy(dAtA[i:], m.MemoryOptimizerTargetNodeNameScope)
		i = encodeVarintRewriterConfig(dAtA, i, uint64(len(m.MemoryOptimizerTargetNodeNameScope)))
		i--
		dAtA[i] = 0x32
	}
	if m.AutoParallel != nil {
		{
			size, err := m.AutoParallel.MarshalToSizedBuffer(dAtA[:i])
			if err != nil {
				return 0, err
			}
			i -= size
			i = encodeVarintRewriterConfig(dAtA, i, uint64(size))
		}
		i--
		dAtA[i] = 0x2a
	}
	if m.MemoryOptimization != 0 {
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.MemoryOptimization))
		i--
		dAtA[i] = 0x20
	}
	if m.ConstantFolding != 0 {
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.ConstantFolding))
		i--
		dAtA[i] = 0x18
	}
	if m.DisableModelPruning {
		i--
		if m.DisableModelPruning {
			dAtA[i] = 1
		} else {
			dAtA[i] = 0
		}
		i--
		dAtA[i] = 0x10
	}
	if m.LayoutOptimizer != 0 {
		i = encodeVarintRewriterConfig(dAtA, i, uint64(m.LayoutOptimizer))
		i--
		dAtA[i] = 0x8
	}
	return len(dAtA) - i, nil
}

func (m *RewriterConfig_CustomGraphOptimizer) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalToSizedBuffer(dAtA[:size])
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *RewriterConfig_CustomGraphOptimizer) MarshalTo(dAtA []byte) (int, error) {
	size := m.Size()
	return m.MarshalToSizedBuffer(dAtA[:size])
}

func (m *RewriterConfig_CustomGraphOptimizer) MarshalToSizedBuffer(dAtA []byte) (int, error) {
	i := len(dAtA)
	_ = i
	var l int
	_ = l
	if len(m.ParameterMap) > 0 {
		for k := range m.ParameterMap {
			v := m.ParameterMap[k]
			baseI := i
			if v != nil {
				{
					size, err := v.MarshalToSizedBuffer(dAtA[:i])
					if err != nil {
						return 0, err
					}
					i -= size
					i = encodeVarintRewriterConfig(dAtA, i, uint64(size))
				}
				i--
				dAtA[i] = 0x12
			}
			i -= len(k)
			copy(dAtA[i:], k)
			i = encodeVarintRewriterConfig(dAtA, i, uint64(len(k)))
			i--
			dAtA[i] = 0xa
			i = encodeVarintRewriterConfig(dAtA, i, uint64(baseI-i))
			i--
			dAtA[i] = 0x12
		}
	}
	if len(m.Name) > 0 {
		i -= len(m.Name)
		copy(dAtA[i:], m.Name)
		i = encodeVarintRewriterConfig(dAtA, i, uint64(len(m.Name)))
		i--
		dAtA[i] = 0xa
	}
	return len(dAtA) - i, nil
}

func encodeVarintRewriterConfig(dAtA []byte, offset int, v uint64) int {
	offset -= sovRewriterConfig(v)
	base := offset
	for v >= 1<<7 {
		dAtA[offset] = uint8(v&0x7f | 0x80)
		v >>= 7
		offset++
	}
	dAtA[offset] = uint8(v)
	return base
}
func (m *AutoParallelOptions) Size() (n int) {
	if m == nil {
		return 0
	}
	var l int
	_ = l
	if m.Enable {
		n += 2
	}
	if m.NumReplicas != 0 {
		n += 1 + sovRewriterConfig(uint64(m.NumReplicas))
	}
	return n
}

func (m *ScopedAllocatorOptions) Size() (n int) {
	if m == nil {
		return 0
	}
	var l int
	_ = l
	if len(m.EnableOp) > 0 {
		for _, s := range m.EnableOp {
			l = len(s)
			n += 1 + l + sovRewriterConfig(uint64(l))
		}
	}
	return n
}

func (m *RewriterConfig) Size() (n int) {
	if m == nil {
		return 0
	}
	var l int
	_ = l
	if m.LayoutOptimizer != 0 {
		n += 1 + sovRewriterConfig(uint64(m.LayoutOptimizer))
	}
	if m.DisableModelPruning {
		n += 2
	}
	if m.ConstantFolding != 0 {
		n += 1 + sovRewriterConfig(uint64(m.ConstantFolding))
	}
	if m.MemoryOptimization != 0 {
		n += 1 + sovRewriterConfig(uint64(m.MemoryOptimization))
	}
	if m.AutoParallel != nil {
		l = m.AutoParallel.Size()
		n += 1 + l + sovRewriterConfig(uint64(l))
	}
	l = len(m.MemoryOptimizerTargetNodeNameScope)
	if l > 0 {
		n += 1 + l + sovRewriterConfig(uint64(l))
	}
	if m.ArithmeticOptimization != 0 {
		n += 1 + sovRewriterConfig(uint64(m.ArithmeticOptimization))
	}
	if m.DependencyOptimization != 0 {
		n += 1 + sovRewriterConfig(uint64(m.DependencyOptimization))
	}
	if m.LoopOptimization != 0 {
		n += 1 + sovRewriterConfig(uint64(m.LoopOptimization))
	}
	if m.FunctionOptimization != 0 {
		n += 1 + sovRewriterConfig(uint64(m.FunctionOptimization))
	}
	if m.DebugStripper != 0 {
		n += 1 + sovRewriterConfig(uint64(m.DebugStripper))
	}
	if m.MetaOptimizerIterations != 0 {
		n += 1 + sovRewriterConfig(uint64(m.MetaOptimizerIterations))
	}
	if m.ShapeOptimization != 0 {
		n += 1 + sovRewriterConfig(uint64(m.ShapeOptimization))
	}
	if m.Remapping != 0 {
		n += 1 + sovRewriterConfig(uint64(m.Remapping))
	}
	if m.ScopedAllocatorOptimization != 0 {
		n += 1 + sovRewriterConfig(uint64(m.ScopedAllocatorOptimization))
	}
	if m.ScopedAllocatorOpts != nil {
		l = m.ScopedAllocatorOpts.Size()
		n += 2 + l + sovRewriterConfig(uint64(l))
	}
	if m.MinGraphNodes != 0 {
		n += 2 + sovRewriterConfig(uint64(m.MinGraphNodes))
	}
	if m.PinToHostOptimization != 0 {
		n += 2 + sovRewriterConfig(uint64(m.PinToHostOptimization))
	}
	if m.DisableMetaOptimizer {
		n += 3
	}
	if m.MetaOptimizerTimeoutMs != 0 {
		n += 2 + sovRewriterConfig(uint64(m.MetaOptimizerTimeoutMs))
	}
	if m.FailOnOptimizerErrors {
		n += 3
	}
	if m.ImplementationSelector != 0 {
		n += 2 + sovRewriterConfig(uint64(m.ImplementationSelector))
	}
	if m.AutoMixedPrecision != 0 {
		n += 2 + sovRewriterConfig(uint64(m.AutoMixedPrecision))
	}
	if m.CommonSubgraphElimination != 0 {
		n += 2 + sovRewriterConfig(uint64(m.CommonSubgraphElimination))
	}
	if m.AutoMixedPrecisionMkl != 0 {
		n += 2 + sovRewriterConfig(uint64(m.AutoMixedPrecisionMkl))
	}
	if m.ExperimentalDisableCompressedTensorOptimization {
		n += 3
	}
	if m.ExperimentalDisableFoldingQuantizationEmulation {
		n += 3
	}
	if m.UsePluginOptimizers != 0 {
		n += 2 + sovRewriterConfig(uint64(m.UsePluginOptimizers))
	}
	if m.CpuLayoutConversion != 0 {
		n += 2 + sovRewriterConfig(uint64(m.CpuLayoutConversion))
	}
	if len(m.Optimizers) > 0 {
		for _, s := range m.Optimizers {
			l = len(s)
			n += 2 + l + sovRewriterConfig(uint64(l))
		}
	}
	if len(m.CustomOptimizers) > 0 {
		for _, e := range m.CustomOptimizers {
			l = e.Size()
			n += 2 + l + sovRewriterConfig(uint64(l))
		}
	}
	if m.InterOptimizerVerifierConfig != nil {
		l = m.InterOptimizerVerifierConfig.Size()
		n += 2 + l + sovRewriterConfig(uint64(l))
	}
	if m.PostOptimizationVerifierConfig != nil {
		l = m.PostOptimizationVerifierConfig.Size()
		n += 2 + l + sovRewriterConfig(uint64(l))
	}
	return n
}

func (m *RewriterConfig_CustomGraphOptimizer) Size() (n int) {
	if m == nil {
		return 0
	}
	var l int
	_ = l
	l = len(m.Name)
	if l > 0 {
		n += 1 + l + sovRewriterConfig(uint64(l))
	}
	if len(m.ParameterMap) > 0 {
		for k, v := range m.ParameterMap {
			_ = k
			_ = v
			l = 0
			if v != nil {
				l = v.Size()
				l += 1 + sovRewriterConfig(uint64(l))
			}
			mapEntrySize := 1 + len(k) + sovRewriterConfig(uint64(len(k))) + l
			n += mapEntrySize + 1 + sovRewriterConfig(uint64(mapEntrySize))
		}
	}
	return n
}

func sovRewriterConfig(x uint64) (n int) {
	return (math_bits.Len64(x|1) + 6) / 7
}
func sozRewriterConfig(x uint64) (n int) {
	return sovRewriterConfig(uint64((x << 1) ^ uint64((int64(x) >> 63))))
}
func (m *AutoParallelOptions) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowRewriterConfig
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= uint64(b&0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: AutoParallelOptions: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: AutoParallelOptions: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Enable", wireType)
			}
			var v int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				v |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			m.Enable = bool(v != 0)
		case 2:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field NumReplicas", wireType)
			}
			m.NumReplicas = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.NumReplicas |= int32(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		default:
			iNdEx = preIndex
			skippy, err := skipRewriterConfig(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			if (iNdEx + skippy) < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *ScopedAllocatorOptions) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowRewriterConfig
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= uint64(b&0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: ScopedAllocatorOptions: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: ScopedAllocatorOptions: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field EnableOp", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			postIndex := iNdEx + intStringLen
			if postIndex < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.EnableOp = append(m.EnableOp, string(dAtA[iNdEx:postIndex]))
			iNdEx = postIndex
		default:
			iNdEx = preIndex
			skippy, err := skipRewriterConfig(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			if (iNdEx + skippy) < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *RewriterConfig) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowRewriterConfig
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= uint64(b&0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: RewriterConfig: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: RewriterConfig: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field LayoutOptimizer", wireType)
			}
			m.LayoutOptimizer = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.LayoutOptimizer |= RewriterConfig_Toggle(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 2:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field DisableModelPruning", wireType)
			}
			var v int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				v |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			m.DisableModelPruning = bool(v != 0)
		case 3:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ConstantFolding", wireType)
			}
			m.ConstantFolding = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.ConstantFolding |= RewriterConfig_Toggle(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 4:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field MemoryOptimization", wireType)
			}
			m.MemoryOptimization = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.MemoryOptimization |= RewriterConfig_MemOptType(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 5:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field AutoParallel", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			postIndex := iNdEx + msglen
			if postIndex < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			if m.AutoParallel == nil {
				m.AutoParallel = &AutoParallelOptions{}
			}
			if err := m.AutoParallel.Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		case 6:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field MemoryOptimizerTargetNodeNameScope", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			postIndex := iNdEx + intStringLen
			if postIndex < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.MemoryOptimizerTargetNodeNameScope = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		case 7:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ArithmeticOptimization", wireType)
			}
			m.ArithmeticOptimization = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.ArithmeticOptimization |= RewriterConfig_Toggle(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 8:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field DependencyOptimization", wireType)
			}
			m.DependencyOptimization = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.DependencyOptimization |= RewriterConfig_Toggle(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 9:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field LoopOptimization", wireType)
			}
			m.LoopOptimization = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.LoopOptimization |= RewriterConfig_Toggle(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 10:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field FunctionOptimization", wireType)
			}
			m.FunctionOptimization = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.FunctionOptimization |= RewriterConfig_Toggle(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 11:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field DebugStripper", wireType)
			}
			m.DebugStripper = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.DebugStripper |= RewriterConfig_Toggle(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 12:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field MetaOptimizerIterations", wireType)
			}
			m.MetaOptimizerIterations = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.MetaOptimizerIterations |= RewriterConfig_NumIterationsType(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 13:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ShapeOptimization", wireType)
			}
			m.ShapeOptimization = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.ShapeOptimization |= RewriterConfig_Toggle(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 14:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Remapping", wireType)
			}
			m.Remapping = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.Remapping |= RewriterConfig_Toggle(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 15:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ScopedAllocatorOptimization", wireType)
			}
			m.ScopedAllocatorOptimization = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.ScopedAllocatorOptimization |= RewriterConfig_Toggle(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 16:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field ScopedAllocatorOpts", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			postIndex := iNdEx + msglen
			if postIndex < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			if m.ScopedAllocatorOpts == nil {
				m.ScopedAllocatorOpts = &ScopedAllocatorOptions{}
			}
			if err := m.ScopedAllocatorOpts.Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		case 17:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field MinGraphNodes", wireType)
			}
			m.MinGraphNodes = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.MinGraphNodes |= int32(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 18:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field PinToHostOptimization", wireType)
			}
			m.PinToHostOptimization = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.PinToHostOptimization |= RewriterConfig_Toggle(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 19:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field DisableMetaOptimizer", wireType)
			}
			var v int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				v |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			m.DisableMetaOptimizer = bool(v != 0)
		case 20:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field MetaOptimizerTimeoutMs", wireType)
			}
			m.MetaOptimizerTimeoutMs = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.MetaOptimizerTimeoutMs |= int64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 21:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field FailOnOptimizerErrors", wireType)
			}
			var v int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				v |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			m.FailOnOptimizerErrors = bool(v != 0)
		case 22:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ImplementationSelector", wireType)
			}
			m.ImplementationSelector = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.ImplementationSelector |= RewriterConfig_Toggle(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 23:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field AutoMixedPrecision", wireType)
			}
			m.AutoMixedPrecision = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.AutoMixedPrecision |= RewriterConfig_Toggle(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 24:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field CommonSubgraphElimination", wireType)
			}
			m.CommonSubgraphElimination = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.CommonSubgraphElimination |= RewriterConfig_Toggle(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 25:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field AutoMixedPrecisionMkl", wireType)
			}
			m.AutoMixedPrecisionMkl = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.AutoMixedPrecisionMkl |= RewriterConfig_Toggle(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 26:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ExperimentalDisableCompressedTensorOptimization", wireType)
			}
			var v int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				v |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			m.ExperimentalDisableCompressedTensorOptimization = bool(v != 0)
		case 27:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ExperimentalDisableFoldingQuantizationEmulation", wireType)
			}
			var v int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				v |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			m.ExperimentalDisableFoldingQuantizationEmulation = bool(v != 0)
		case 28:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field UsePluginOptimizers", wireType)
			}
			m.UsePluginOptimizers = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.UsePluginOptimizers |= RewriterConfig_Toggle(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 50:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field CpuLayoutConversion", wireType)
			}
			m.CpuLayoutConversion = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.CpuLayoutConversion |= RewriterConfig_CpuLayout(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 100:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Optimizers", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			postIndex := iNdEx + intStringLen
			if postIndex < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.Optimizers = append(m.Optimizers, string(dAtA[iNdEx:postIndex]))
			iNdEx = postIndex
		case 200:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field CustomOptimizers", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			postIndex := iNdEx + msglen
			if postIndex < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.CustomOptimizers = append(m.CustomOptimizers, &RewriterConfig_CustomGraphOptimizer{})
			if err := m.CustomOptimizers[len(m.CustomOptimizers)-1].Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		case 300:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field InterOptimizerVerifierConfig", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			postIndex := iNdEx + msglen
			if postIndex < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			if m.InterOptimizerVerifierConfig == nil {
				m.InterOptimizerVerifierConfig = &VerifierConfig{}
			}
			if err := m.InterOptimizerVerifierConfig.Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		case 301:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field PostOptimizationVerifierConfig", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			postIndex := iNdEx + msglen
			if postIndex < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			if m.PostOptimizationVerifierConfig == nil {
				m.PostOptimizationVerifierConfig = &VerifierConfig{}
			}
			if err := m.PostOptimizationVerifierConfig.Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		default:
			iNdEx = preIndex
			skippy, err := skipRewriterConfig(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			if (iNdEx + skippy) < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *RewriterConfig_CustomGraphOptimizer) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowRewriterConfig
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= uint64(b&0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: CustomGraphOptimizer: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: CustomGraphOptimizer: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Name", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			postIndex := iNdEx + intStringLen
			if postIndex < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.Name = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		case 2:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field ParameterMap", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			postIndex := iNdEx + msglen
			if postIndex < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			if m.ParameterMap == nil {
				m.ParameterMap = make(map[string]*framework.AttrValue)
			}
			var mapkey string
			var mapvalue *framework.AttrValue
			for iNdEx < postIndex {
				entryPreIndex := iNdEx
				var wire uint64
				for shift := uint(0); ; shift += 7 {
					if shift >= 64 {
						return ErrIntOverflowRewriterConfig
					}
					if iNdEx >= l {
						return io.ErrUnexpectedEOF
					}
					b := dAtA[iNdEx]
					iNdEx++
					wire |= uint64(b&0x7F) << shift
					if b < 0x80 {
						break
					}
				}
				fieldNum := int32(wire >> 3)
				if fieldNum == 1 {
					var stringLenmapkey uint64
					for shift := uint(0); ; shift += 7 {
						if shift >= 64 {
							return ErrIntOverflowRewriterConfig
						}
						if iNdEx >= l {
							return io.ErrUnexpectedEOF
						}
						b := dAtA[iNdEx]
						iNdEx++
						stringLenmapkey |= uint64(b&0x7F) << shift
						if b < 0x80 {
							break
						}
					}
					intStringLenmapkey := int(stringLenmapkey)
					if intStringLenmapkey < 0 {
						return ErrInvalidLengthRewriterConfig
					}
					postStringIndexmapkey := iNdEx + intStringLenmapkey
					if postStringIndexmapkey < 0 {
						return ErrInvalidLengthRewriterConfig
					}
					if postStringIndexmapkey > l {
						return io.ErrUnexpectedEOF
					}
					mapkey = string(dAtA[iNdEx:postStringIndexmapkey])
					iNdEx = postStringIndexmapkey
				} else if fieldNum == 2 {
					var mapmsglen int
					for shift := uint(0); ; shift += 7 {
						if shift >= 64 {
							return ErrIntOverflowRewriterConfig
						}
						if iNdEx >= l {
							return io.ErrUnexpectedEOF
						}
						b := dAtA[iNdEx]
						iNdEx++
						mapmsglen |= int(b&0x7F) << shift
						if b < 0x80 {
							break
						}
					}
					if mapmsglen < 0 {
						return ErrInvalidLengthRewriterConfig
					}
					postmsgIndex := iNdEx + mapmsglen
					if postmsgIndex < 0 {
						return ErrInvalidLengthRewriterConfig
					}
					if postmsgIndex > l {
						return io.ErrUnexpectedEOF
					}
					mapvalue = &framework.AttrValue{}
					if err := mapvalue.Unmarshal(dAtA[iNdEx:postmsgIndex]); err != nil {
						return err
					}
					iNdEx = postmsgIndex
				} else {
					iNdEx = entryPreIndex
					skippy, err := skipRewriterConfig(dAtA[iNdEx:])
					if err != nil {
						return err
					}
					if skippy < 0 {
						return ErrInvalidLengthRewriterConfig
					}
					if (iNdEx + skippy) > postIndex {
						return io.ErrUnexpectedEOF
					}
					iNdEx += skippy
				}
			}
			m.ParameterMap[mapkey] = mapvalue
			iNdEx = postIndex
		default:
			iNdEx = preIndex
			skippy, err := skipRewriterConfig(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if skippy < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			if (iNdEx + skippy) < 0 {
				return ErrInvalidLengthRewriterConfig
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func skipRewriterConfig(dAtA []byte) (n int, err error) {
	l := len(dAtA)
	iNdEx := 0
	depth := 0
	for iNdEx < l {
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return 0, ErrIntOverflowRewriterConfig
			}
			if iNdEx >= l {
				return 0, io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		wireType := int(wire & 0x7)
		switch wireType {
		case 0:
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return 0, ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return 0, io.ErrUnexpectedEOF
				}
				iNdEx++
				if dAtA[iNdEx-1] < 0x80 {
					break
				}
			}
		case 1:
			iNdEx += 8
		case 2:
			var length int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return 0, ErrIntOverflowRewriterConfig
				}
				if iNdEx >= l {
					return 0, io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				length |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if length < 0 {
				return 0, ErrInvalidLengthRewriterConfig
			}
			iNdEx += length
		case 3:
			depth++
		case 4:
			if depth == 0 {
				return 0, ErrUnexpectedEndOfGroupRewriterConfig
			}
			depth--
		case 5:
			iNdEx += 4
		default:
			return 0, fmt.Errorf("proto: illegal wireType %d", wireType)
		}
		if iNdEx < 0 {
			return 0, ErrInvalidLengthRewriterConfig
		}
		if depth == 0 {
			return iNdEx, nil
		}
	}
	return 0, io.ErrUnexpectedEOF
}

var (
	ErrInvalidLengthRewriterConfig        = fmt.Errorf("proto: negative length found during unmarshaling")
	ErrIntOverflowRewriterConfig          = fmt.Errorf("proto: integer overflow")
	ErrUnexpectedEndOfGroupRewriterConfig = fmt.Errorf("proto: unexpected end of group")
)
